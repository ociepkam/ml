{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstęp do sieci neuronowych\n",
    "\n",
    "\n",
    "<img src=\"figures/L9/cat_neuron.jpg\" width=500>\n",
    "\n",
    "Ref: \n",
    "\n",
    "* http://cs231n.github.io/optimization-1/ \n",
    "\n",
    "* https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.3%20Introduction%20-%20Keras.ipynb\n",
    "\n",
    "* http://colah.github.io/posts/2014-10-Visualizing-MNIST/\n",
    "\n",
    "* https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/neural_net_implementation/neural_network_implementation_part02.ipynb\n",
    "\n",
    "\n",
    "Na dzisiejszych zajęciach omówimy jak można zaimplementować jako sieć neuronową znany nam model regresji logistycznej, najpierw bardziej ręcznie, a następnie z użyciem pakietu keras. Omówimy na końcu spojrzenie na sieci neuronowe z perspektywy uczenia reprezentacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ea4bd1334c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;31m#import tqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'KERAS_BACKEND'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'theano'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "from matplotlib import cm #\n",
    "import os\n",
    "import tqdm\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "%matplotlib inline\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Small MNIST \n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "sm_train = mnist_x_train[0:500].astype('float32') / 255.\n",
    "sm_y_train = mnist_y_train[0:500]\n",
    "sm_test = mnist_x_test[0:500].astype('float32') / 255.\n",
    "sm_y_test = mnist_y_test[0:500]\n",
    "sm_train = sm_train.reshape((len(sm_train), np.prod(sm_train.shape[1:])))\n",
    "sm_test = sm_test.reshape((len(sm_test), np.prod(sm_test.shape[1:])))\n",
    "print sm_train.shape\n",
    "print sm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not keras.__version__[0] == '2':\n",
    "    raise Exception(\"Prosze zainstalowac keras>=2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def nn(x, w): \n",
    "    return logistic(x.dot(w.T))\n",
    "\n",
    "def nn_predict(x,w): \n",
    "    return np.around(nn(x,w))\n",
    "\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 100  # The number of sample in each class\n",
    "red_mean = [-1,0]  # The mean of the red class\n",
    "blue_mean = [1,0]  # The mean of the blue class\n",
    "std_dev = 1.2  # standard deviation of both classes\n",
    "# Generate samples from both classes\n",
    "x_red = np.random.randn(nb_of_samples_per_class, 2) * std_dev + red_mean\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 2) * std_dev + blue_mean\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of output variables t\n",
    "X_toy = np.vstack((x_red, x_blue))\n",
    "t_toy = np.vstack((np.zeros((nb_of_samples_per_class,1)), np.ones((nb_of_samples_per_class,1))))\n",
    "\n",
    "toy_train, toy_test, toy_y_train, toy_y_test = train_test_split(X_toy, t_toy, test_size=0.15)\n",
    "\n",
    "# Przyda sie do sekcji o kerasie\n",
    "toy_y_train_one_hot = keras.utils.to_categorical(toy_y_train)\n",
    "toy_y_test_one_hot = keras.utils.to_categorical(toy_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot both classes on the x1, x2 plane\n",
    "plt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\n",
    "plt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('$x_1$', fontsize=15)\n",
    "plt.ylabel('$x_2$', fontsize=15)\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.title('red vs. blue classes in the input space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siec Neuronowa, a regresja logistyczna\n",
    "\n",
    "Przypomnijmy sobie co robi regresja logistyczna. Z L7:\n",
    "\n",
    "Przekształcając $$ \\log(o) = \\sum \\theta_i x_i $$ otrzymujemy *bezpośrednio*, że $$ p(y | x) = \\mbox{sigmoid}(\\sum \\theta_i x_i) $$, gdzie $sigmoid(a) = \\frac{1}{1 + \\exp(-a)}$.\n",
    "\n",
    "Można ostatni wzór zapisać w postaci \"sieci neuronowej\":\n",
    "\n",
    "<center><img width=500 src=\"figures/L9/logreg.png\"></center>\n",
    "\n",
    "To co pozostaje to jak wyznaczyć wagi przez optymalizacją. Przypomnijmy, że szukamy parametrów optymalizujących log-likelihood $$ LL(\\hat y, y) = CE(\\hat y, y) = \\sum_{i=1}^{N} y \\log\\hat(y) $$. W regresji liniowej było prosto!\n",
    "\n",
    "[Opisać/wyjaśnic cross entropy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czego szuka regresja logistyczna\n",
    "\n",
    "Regresja logistyczna dostaje kare za najdrobniejsza pomyłkę. Jeśli dla kasy 1 mówi z pewnością 99%, to dalej będzie niezerowy koszt:\n",
    "    \n",
    "<img width=500 src=\"figures/L9/logreg_vs_svm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizacja\n",
    "\n",
    "Jak dotad nie rozpatrywaliśmy optymalizacji. Regresja liniowa ma rozwiązanie \"zamknięte\", ale nie wiemy jak optymalizować błąd regresji logistycznej!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^* = argmax_{\\theta} L(p( \\hat y | x, \\theta), y) = CE(p(\\hat y | x, \\theta), y) = \\sum_{i=1}^{N} y \\log\\hat(y) $$.\n",
    "\n",
    "Dla klas binarnych:\n",
    "\n",
    "$$ \\sum_{i=1}^N y_i \\log \\hat y_i - (1 - y_i) \\log (1 - \\hat y_i ) $$\n",
    "\n",
    "Jak znaleźć $\\theta^*$? Użyjemy metody gradientowej:\n",
    "\n",
    "$$ \\theta^{t+1} = \\theta^{t} - \\alpha \\frac{\\partial L}{\\partial \\theta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja kosztu nie jest już taka prosta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the cost in function of the weights\n",
    "# Define a vector of weights for which we want to plot the cost\n",
    "nb_of_ws = 100 # compute the cost nb_of_ws times in each dimension\n",
    "ws1 = np.linspace(-5, 5, num=nb_of_ws) # weight 1\n",
    "ws2 = np.linspace(-5, 5, num=nb_of_ws) # weight 2\n",
    "ws_x, ws_y = np.meshgrid(ws1, ws2) # generate grid\n",
    "cost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n",
    "\n",
    "# Fill the cost matrix for each combination of weights\n",
    "for i in range(nb_of_ws):\n",
    "    for j in range(nb_of_ws):\n",
    "        cost_ws[i,j] = cost(nn(toy_train, np.asmatrix([ws_x[i,j], ws_y[i,j]])) , toy_y_train)\n",
    "        \n",
    "# Plot the cost function surface\n",
    "plt.contourf(ws_x, ws_y, cost_ws, 20, cmap=cm.pink)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.xlabel('$w_1$', fontsize=15)\n",
    "plt.ylabel('$w_2$', fontsize=15)\n",
    "plt.title('Cost function surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja SGD\n",
    "\n",
    "[Wyprowadzić na tablicy wzór na dLL/dtheta_i, jak to sie ma do backprop]\n",
    "\n",
    "Jak właśnie pokazaliśmy $$ \\frac{\\partial L}{\\partial \\theta_i} = x_j (\\hat y_i - t_i) $$\n",
    "\n",
    "[Interpretacja geometryczna gradientu regresji logistycznej]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1, Nauka regresji logistycznej przez SGD [2pkt]\n",
    "\n",
    "1. Zaimplementuj SGD dla zbioru danych toy. \n",
    "2. Wygeneruj wizualizacje procesu uczenia (ostatnia komórka)\n",
    "3. Narysuj wizualizacje dla learning_rate=0.05, 0.1 i 0.01\n",
    "4. Dla jakiego learning_rate osiągamy najlepszą dokladnosc na (toy_test, toy_y_test)?\n",
    "\n",
    "Co powinno wyjść dla odpowiednio 0.01 oraz 0.1:\n",
    "\n",
    "<img width=200 src=\"figures/L9/lr0.01.png\">\n",
    "\n",
    "<img width=200 src=\"figures/L9/lr0.1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def gradient(w, x, t):     \n",
    "    preds = sigmoid_activation(x.dot(w.T))\n",
    "    y = np.array([n[0] for n in t])\n",
    "    e = [n[0,0] for n in preds]\n",
    "    error = e - y\n",
    " \n",
    "    gradient = x.T.dot(error)\n",
    "    return np.asmatrix(gradient)\n",
    "    \n",
    "\n",
    "def delta_w(w_k, x, t, learning_rate):\n",
    "    return learning_rate * gradient(w_k,x,t)\n",
    "\n",
    "# Parametry uczenia \n",
    "w = np.asmatrix([-4, -2])\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Uczenie\n",
    "nb_of_iterations = 10 \n",
    "w_iter = [w] \n",
    "for i in range(nb_of_iterations):\n",
    "    dw = delta_w(w, toy_train, toy_y_train, learning_rate) \n",
    "    w = w - dw \n",
    "    w_iter.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the first weight updates on the error surface\n",
    "# Plot the error surface\n",
    "plt.contourf(ws_x, ws_y, cost_ws, 20, alpha=0.9, cmap=cm.pink)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('cost')\n",
    "\n",
    "# Plot the updates\n",
    "for i in range(1, 4): \n",
    "    w1 = w_iter[i-1]\n",
    "    w2 = w_iter[i]\n",
    "    # Plot the weight-cost value and the line that represents the update\n",
    "    plt.plot(w1[0,0], w1[0,1], 'bo')  # Plot the weight cost value\n",
    "    plt.plot([w1[0,0], w2[0,0]], [w1[0,1], w2[0,1]], 'b-')\n",
    "    plt.text(w1[0,0]-0.2, w1[0,1]+0.4, '$w({})$'.format(i), color='b')\n",
    "w1 = w_iter[3]  \n",
    "# Plot the last weight\n",
    "plt.plot(w1[0,0], w1[0,1], 'bo')\n",
    "plt.text(w1[0,0]-0.2, w1[0,1]+0.4, '$w({})$'.format(4), color='b') \n",
    "# Show figure\n",
    "plt.xlabel('$w_1$', fontsize=15)\n",
    "plt.ylabel('$w_2$', fontsize=15)\n",
    "plt.title('Gradient descent updates on cost surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras\n",
    "\n",
    "Sieci neuronowe definiujemy prawie tak prosto jak funkcje w Pythonie, a pochodne (to co liczyłem na tablicy) powinien nam liczyć komputer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = toy_train.shape[1]\n",
    "print(dims, 'dims')\n",
    "nb_classes = toy_y_train_one_hot.shape[1]\n",
    "print(nb_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Definicja modelu\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(dims,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 2. Kompilacja\n",
    "model.compile(optimizer=SGD(lr=0.1), loss='categorical_crossentropy')\n",
    "\n",
    "# 3. Uczenie\n",
    "model.fit(toy_train, toy_y_train_one_hot, epochs=10)\n",
    "\n",
    "# 4. Predykcja\n",
    "toy_y_pred = model.predict(toy_test).argmax(axis=1)\n",
    "toy_y_train_pred = model.predict(toy_train).argmax(axis=1)\n",
    "print model.layers[0].trainable_weights[0].get_value()\n",
    "print np.mean(toy_y_train.reshape(-1,) == toy_y_train_pred)\n",
    "print np.mean(toy_y_test.reshape(-1,) == toy_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2, Proste modyfikacje [2pkt]\n",
    "\n",
    "\n",
    "1. (1pkt) Zmodyfikuj powyższą sieć tak aby osiągnęła 100% na zbiorze trenującym w rozważanym datasecie. Odpowiedz na pytania:\n",
    "   * Czy ten model osiąga lepszy wynik na zbiorze testowym niż regresja logistyczna?\n",
    "   * Czy można osiągnąć lepszy wynik na tym zbiorze danych, nawet jeśli posiadalibyśmy nieskończoną próbkę danych?\n",
    "2. (1pkt) Naucz 2 warstwową sieć na smallMNIST. Czy dla jakiegoś rozmiaru ukrytej warstwy osiąga lepszy wynik niż regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(toy_train, toy_y_train)\n",
    "Z = logreg.predict(toy_train)\n",
    "round(np.mean(toy_y_train.reshape(-1,) == Z), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_the_best_model(test_lr=0.1, test_act='relu', test_nb_classes = [nb_classes], epochs=40, batch_size=10):    \n",
    "    i = 30\n",
    "    # 1. Definicja modelu\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(test_nb_classes[0], input_shape=(dims,), activation=test_act))\n",
    "    for cl in test_nb_classes[1:-1]:\n",
    "        model.add(Dense(cl, activation=test_act)) #, activation=test_act\n",
    "    model.add(Dense(test_nb_classes[-1], activation='softmax'))\n",
    "\n",
    "    # 2. Kompilacja\n",
    "    model.compile(optimizer=SGD(lr=test_lr), loss='categorical_crossentropy')\n",
    "\n",
    "    # 3. Uczenie\n",
    "    model.fit(toy_train[:i], toy_y_train_one_hot[:i], epochs=epochs, verbose=0, batch_size=batch_size) #verbose=0,\n",
    "\n",
    "    # 4. Predykcja\n",
    "    toy_y_pred = model.predict(toy_test).argmax(axis=1)\n",
    "    toy_y_train_pred = model.predict(toy_train[:i]).argmax(axis=1)\n",
    "    #print model.layers[0].trainable_weights[0].get_value()\n",
    "    \n",
    "    if np.mean(toy_y_train.reshape(-1,) == toy_y_train_pred) > 0.8:\n",
    "        print \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"\n",
    "    print round(np.mean(toy_y_train.reshape(-1,)[:i] == toy_y_train_pred), 2), '\\t', test_lr, \"\\t\", test_act, '\\t', test_nb_classes\n",
    "    \n",
    "    #print np.mean(toy_y_test.reshape(-1,) == toy_y_pred)\n",
    "\n",
    "def grid_search(grid):\n",
    "    (keys, values_grid) = zip(*grid.iteritems())\n",
    "    for values in product(*values_grid):\n",
    "        yield dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lr = [0.01]\n",
    "# act = ['relu']\n",
    "# #cl = [[2,4,2,2], [3,4,2,2], [8,4,2,2], [20,4,2,2]]\n",
    "# cl = [[90, 60, 30, 15, 2]]\n",
    "# for d in grid_search({'lr': lr, 'act': act, 'cl': cl}):\n",
    "#     find_the_best_model(test_lr=d['lr'], test_act=d['act'], test_nb_classes=d['cl'], epochs=5000, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszy osiągnięty wynik:\n",
    "\n",
    "0.97 \t0.02 \trelu \t[90, 60, 30, 15, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Regresja logistyczna nie byłaby wstanie tak dobrze rozdzielić danych jak dobrze zaprojektowana sieć neuronowa.\n",
    "\n",
    "b) Nie da się osiągnąć lepszego wyniku niż 100%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(sm_train, sm_y_train)\n",
    "print logreg.score(sm_test, sm_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_the_best_model2(x, y, z, x_test, y_test, test_lr=0.1, test_act='relu', test_nb_classes = [nb_classes], epochs=40, batch_size=10):    \n",
    "    # 1. Definicja modelu\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(test_nb_classes[0], input_shape=(test_nb_classes[0],), activation=test_act))\n",
    "    for cl in test_nb_classes[1:-1]:\n",
    "        model.add(Dense(cl, activation=test_act))\n",
    "    model.add(Dense(test_nb_classes[-1], input_shape=(test_nb_classes[-1],), activation='softmax'))\n",
    "\n",
    "    # 2. Kompilacja\n",
    "    model.compile(optimizer=SGD(lr=1), loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "    # 3. Uczenie\n",
    "    model.fit(sm_train, sm_y_train_one_hot, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    # 4. Predykcja\n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "\n",
    "    print round(np.mean(y_test == y_pred), 2), '\\t', test_lr, \"\\t\", test_act, '\\t', test_nb_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sm_y_train_one_hot = keras.utils.to_categorical(sm_y_train)\n",
    "# lr = [0.02]\n",
    "# act = ['relu']\n",
    "# cl = [[784, 10]]\n",
    "# for d in grid_search({'lr': lr, 'act': act, 'cl': cl}):\n",
    "#     find_the_best_model2(sm_train, sm_y_train, sm_y_train_one_hot, sm_test, sm_y_test, test_lr=d['lr'], test_act=d['act'], test_nb_classes=d['cl'], epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Najlepszy wynik:\n",
    "\n",
    "Epoch 1/50\n",
    "500/500 [==============================] - 21s - loss: 2.3235 - acc: 0.1200\n",
    "Epoch 2/50\n",
    "500/500 [==============================] - 21s - loss: 1.3939 - acc: 0.5660\n",
    "Epoch 3/50\n",
    "500/500 [==============================] - 21s - loss: 2.1761 - acc: 0.4640\n",
    "Epoch 4/50\n",
    "500/500 [==============================] - 21s - loss: 2.7913 - acc: 0.4900\n",
    "Epoch 5/50\n",
    "500/500 [==============================] - 21s - loss: 1.7641 - acc: 0.4200\n",
    "Epoch 6/50\n",
    "500/500 [==============================] - 21s - loss: 1.1253 - acc: 0.7240\n",
    "Epoch 7/50\n",
    "500/500 [==============================] - 20s - loss: 0.6998 - acc: 0.8560\n",
    "Epoch 8/50\n",
    "500/500 [==============================] - 21s - loss: 0.5761 - acc: 0.8760\n",
    "Epoch 9/50\n",
    "500/500 [==============================] - 21s - loss: 0.5747 - acc: 0.8180\n",
    "Epoch 10/50\n",
    "500/500 [==============================] - 21s - loss: 0.7767 - acc: 0.7280\n",
    "Epoch 11/50\n",
    "500/500 [==============================] - 21s - loss: 0.9090 - acc: 0.7120\n",
    "Epoch 12/50\n",
    "500/500 [==============================] - 21s - loss: 0.7720 - acc: 0.7960\n",
    "Epoch 13/50\n",
    "500/500 [==============================] - 21s - loss: 0.5993 - acc: 0.8020\n",
    "Epoch 14/50\n",
    "500/500 [==============================] - 21s - loss: 0.3926 - acc: 0.9220\n",
    "Epoch 15/50\n",
    "500/500 [==============================] - 21s - loss: 0.2875 - acc: 0.9340\n",
    "Epoch 16/50\n",
    "500/500 [==============================] - 21s - loss: 0.2404 - acc: 0.9540\n",
    "Epoch 17/50\n",
    "500/500 [==============================] - 21s - loss: 0.2128 - acc: 0.9540\n",
    "Epoch 18/50\n",
    "500/500 [==============================] - 21s - loss: 0.1920 - acc: 0.9640\n",
    "Epoch 19/50\n",
    "500/500 [==============================] - 21s - loss: 0.1750 - acc: 0.9680\n",
    "Epoch 20/50\n",
    "500/500 [==============================] - 21s - loss: 0.1606 - acc: 0.9700\n",
    "Epoch 21/50\n",
    "500/500 [==============================] - 21s - loss: 0.1479 - acc: 0.9740\n",
    "Epoch 22/50\n",
    "500/500 [==============================] - 21s - loss: 0.1367 - acc: 0.9820\n",
    "Epoch 23/50\n",
    "500/500 [==============================] - 21s - loss: 0.1268 - acc: 0.9840\n",
    "Epoch 24/50\n",
    "500/500 [==============================] - 21s - loss: 0.1179 - acc: 0.9860\n",
    "Epoch 25/50\n",
    "500/500 [==============================] - 21s - loss: 0.1099 - acc: 0.9900\n",
    "Epoch 26/50\n",
    "500/500 [==============================] - 21s - loss: 0.1027 - acc: 0.9900\n",
    "Epoch 27/50\n",
    "500/500 [==============================] - 20s - loss: 0.0962 - acc: 0.9920\n",
    "Epoch 28/50\n",
    "500/500 [==============================] - 20s - loss: 0.0902 - acc: 0.9960\n",
    "Epoch 29/50\n",
    "500/500 [==============================] - 21s - loss: 0.0849 - acc: 0.9980\n",
    "Epoch 30/50\n",
    "500/500 [==============================] - 21s - loss: 0.0799 - acc: 0.9980\n",
    "Epoch 31/50\n",
    "500/500 [==============================] - 21s - loss: 0.0755 - acc: 0.9980\n",
    "Epoch 32/50\n",
    "500/500 [==============================] - 21s - loss: 0.0713 - acc: 1.0000\n",
    "Epoch 33/50\n",
    "500/500 [==============================] - 21s - loss: 0.0676 - acc: 1.0000\n",
    "Epoch 34/50\n",
    "500/500 [==============================] - 21s - loss: 0.0642 - acc: 1.0000\n",
    "Epoch 35/50\n",
    "500/500 [==============================] - 21s - loss: 0.0610 - acc: 1.0000\n",
    "Epoch 36/50\n",
    "500/500 [==============================] - 21s - loss: 0.0581 - acc: 1.0000\n",
    "Epoch 37/50\n",
    "500/500 [==============================] - 21s - loss: 0.0554 - acc: 1.0000\n",
    "Epoch 38/50\n",
    "500/500 [==============================] - 21s - loss: 0.0529 - acc: 1.0000\n",
    "Epoch 39/50\n",
    "500/500 [==============================] - 21s - loss: 0.0506 - acc: 1.0000\n",
    "Epoch 40/50\n",
    "500/500 [==============================] - 21s - loss: 0.0484 - acc: 1.0000\n",
    "Epoch 41/50\n",
    "500/500 [==============================] - 21s - loss: 0.0464 - acc: 1.0000\n",
    "Epoch 42/50\n",
    "500/500 [==============================] - 21s - loss: 0.0445 - acc: 1.0000\n",
    "Epoch 43/50\n",
    "500/500 [==============================] - 21s - loss: 0.0428 - acc: 1.0000\n",
    "Epoch 44/50\n",
    "500/500 [==============================] - 21s - loss: 0.0412 - acc: 1.0000\n",
    "Epoch 45/50\n",
    "500/500 [==============================] - 21s - loss: 0.0397 - acc: 1.0000\n",
    "Epoch 46/50\n",
    "500/500 [==============================] - 21s - loss: 0.0382 - acc: 1.0000\n",
    "Epoch 47/50\n",
    "500/500 [==============================] - 21s - loss: 0.0369 - acc: 1.0000\n",
    "Epoch 48/50\n",
    "500/500 [==============================] - 21s - loss: 0.0356 - acc: 1.0000\n",
    "Epoch 49/50\n",
    "500/500 [==============================] - 21s - loss: 0.0344 - acc: 1.0000\n",
    "Epoch 50/50\n",
    "500/500 [==============================] - 21s - loss: 0.0333 - acc: 1.0000\n",
    "\n",
    "0.86 \t0.02 \trelu \t[784, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z tego wynika, że sieć osiąga lepsze wynika np. dla sieci o warstwach z neuronami 784, 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inne sieci neuronowe\n",
    "\n",
    "Najprostsze co możemy zrobić to dodać warstwy. Po co omówimy w następnym rozdziale.\n",
    "\n",
    "<img width=600 src=\"figures/L9/zoo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Znaczenie reprezentacji, kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najproszym argumentem po co te sieci jest możliwość uczenia się lepszych reprezentacji niż pixele, czy ręczne cechy w zbiorze danych breast cancer. Omówimy to na przykładzie zbiorów danych: iris i MNIST oraz klasyczngo modelu kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruchomimy najpierw model kNN na danych iris\n",
    "\n",
    "<img width=600 src=\"https://sebastianraschka.com/images/blog/2015/principal_component_analysis_files/iris.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(iris.data[iris['target']==0, 0], iris.data[iris['target']==0, 1], color='r')\n",
    "plt.scatter(iris.data[iris['target']==1, 0], iris.data[iris['target']==1, 1], color='g')\n",
    "plt.scatter(iris.data[iris['target']==2, 0], iris.data[iris['target']==2, 1], color='b')\n",
    "plt.xlabel(iris['feature_names'][0])\n",
    "plt.ylabel(iris['feature_names'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_pred(x, x_train, y_train):\n",
    "    current_best = -1, 100000\n",
    "    for id in range(len(x_train)):\n",
    "        dist = np.linalg.norm(x - x_train[id])\n",
    "        if dist < current_best[1]:\n",
    "            current_best = (id, dist)\n",
    "    return y_train[current_best[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN na Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_pred = []\n",
    "for x in tqdm.tqdm(X_test, total=len(X_test)):\n",
    "    Y_test_pred.append(knn_pred(x, X_train, y_train))\n",
    "print str(int(100*np.mean(np.array(Y_test_pred) == y_test))),\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small MNIST\n",
    "\n",
    "Na Iris nasz prosty model osiąga 91%. Co sie dzieje na small MNIST?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_pred = []\n",
    "for x in tqdm.tqdm(sm_test, total=len(sm_test)):\n",
    "    Y_test_pred.append(knn_pred(x, x_train=sm_train, y_train=sm_y_train))\n",
    "print str(int(100*np.mean(np.array(Y_test_pred) == sm_y_test))),\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co sie stalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_test = 15\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.suptitle(\"Pytanie\")\n",
    "plt.imshow(sm_test[id_test].reshape(28, 28), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_dists = [np.linalg.norm(sm_test[id_test].reshape(-1,) - x) for x in sm_train]\n",
    "all_dists = np.argsort(all_dists)\n",
    "\n",
    "f, ax = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "for id, i in enumerate(all_dists[0:3]):\n",
    "    ax[id].imshow(sm_train[all_dists[id]].reshape(28, 28), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nauka reprezentacji\n",
    "\n",
    "Ref: http://colah.github.io/posts/2014-10-Visualizing-MNIST/\n",
    "\n",
    "Czlowiek uczy się reprezentacji - widzimy na poziomie krawędzi i kształtów, a nie pixeli!\n",
    "\n",
    "<img width=500 src=\"figures/L9/cat_experiment_no_reference.png\">\n",
    "\n",
    "Zastanówmy się wspólnie jaka reprezentacja byłaby fajna? Główna idea Representation Learning, może będzie o tym więcej wkrótce:\n",
    "\n",
    "<img width=600 src=\"figures/L9/autoencoder_schema.jpg\">\n",
    "\n",
    "### PCA\n",
    "\n",
    "Na obecną chwilę użyjemy PCA, o którym prawdopodobnie będzie dużo więcej w przyszłości. \n",
    "\n",
    "<img src=\"figures/L9/pca.png\">\n",
    "\n",
    "(ilustracja pochodzi z tutorialu na temat PCA oraz ICA w sklearn, http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = RandomizedPCA(n_components=8, iterated_power=15, whiten=True)\n",
    "fig, plot = plt.subplots()\n",
    "fig.set_size_inches(8, 8)\n",
    "plt.prism()\n",
    "\n",
    "X_transformed = pca.fit_transform(mnist_x_train.reshape(mnist_x_train.shape[0], -1))\n",
    "plot.scatter(X_transformed[:, 0], X_transformed[:, 1], c=mnist_y_train)\n",
    "plot.set_xticks(())\n",
    "plot.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8 * 20))\n",
    "plt.imshow(pca.components_.reshape((8 * 28, 28)).T, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku twarzy PCA znajdzie bardzo ciekawy zestaw \"cech\", czy \"wektorów bazowych\":\n",
    "\n",
    "<img width=400 src=\"figures/L9/eigenfaces.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3, czy cechy pomagają w kNN [1pkt]\n",
    "\n",
    "1. Naucz PCA jedynie na cyfrach 5 i załącz analogiczny plot jak powyżej\n",
    "\n",
    "2. Użyj nauczonych cech (na całym zbiorze mnist) przez PCA do klasyfikacji kNN na zbiorze small MNIST. Jaki można osiągnąć wynik? Użyj KNeighborsClassifier z pakietu sklearn. Dopasuj parametry PCA oraz tak KNeighborsClassifier aby osiągnąć najlepszy wynik na zbiorze sm_test, wymagany jest wynik powyżej 80% na zbiorze testowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = [idx for idx, val in enumerate(mnist_y_train) if val == 5]\n",
    "mnist_x_train_5 = mnist_x_train[idx]\n",
    "mnist_y_train_5 = mnist_y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = RandomizedPCA(n_components=8, iterated_power=15, whiten=True)\n",
    "fig, plot = plt.subplots()\n",
    "fig.set_size_inches(8, 8)\n",
    "plt.prism()\n",
    "X_transformed = pca.fit_transform(mnist_x_train_5.reshape(mnist_x_train_5.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8* 20))\n",
    "plt.imshow(pca.components_.reshape((8 * 28, 28)).T, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca = RandomizedPCA(n_components=22, iterated_power=7, whiten=True)\n",
    "\n",
    "\n",
    "X_transformed = pca.fit_transform(sm_train.reshape(sm_train.shape[0], -1))\n",
    "knn = KNeighborsClassifier(n_neighbors=4, algorithm=\"kd_tree\")\n",
    "knn.fit(X_transformed, sm_y_train)\n",
    "xtest = pca.transform(sm_test)\n",
    "y_pred = knn.predict(xtest)\n",
    "print metrics.accuracy_score(y_pred, sm_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
